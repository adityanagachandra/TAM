import json
import os
import time
from typing import Any, Dict, List
from datetime import datetime
from pathlib import Path
import numpy as np
from benchmark.tasks.base_evaluator import BaseEvaluator
from benchmark.tasks.result_types import EvaluationResult


class TAMEvaluator(BaseEvaluator):
    """
    Evaluator for the Token Activation Map (TAM) Task.
    Evaluates the quality of visual explanations generated by TAM.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.min_iou = config.get("evaluation_criteria", {}).get("min_iou", 0.5)
        self.min_f1_score = config.get("evaluation_criteria", {}).get("min_f1_score", 0.6)
        self.require_all_images = config.get("evaluation_criteria", {}).get("require_all_images", True)
        self.print_task_info()

    def evaluate(self, solution_folder: str, solution_config: Any = None) -> EvaluationResult:
        """
        Evaluate the TAM visual explanation results.
        
        Args:
            solution_folder: Path to the folder containing tam_results.json
            solution_config: Optional ground truth data
            
        Returns:
            EvaluationResult with TAM metrics
        """
        start_time = time.time()
        
        try:
            # Load results from JSON
            solution_file_name = self.config["expected_outputs"]["solution_file"]
            solution_path = os.path.join(solution_folder, solution_file_name)
            
            if not os.path.exists(solution_path):
                return EvaluationResult(
                    task_id=self.config["task_id"],
                    agent_id="unknown",
                    timestamp=datetime.now(),
                    metrics={"error": "Solution file not found"},
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message=f"Solution file {solution_file_name} not found"
                )
            
            with open(solution_path, 'r') as f:
                results = json.load(f)
            
            # Validate results structure
            if "results" not in results:
                return EvaluationResult(
                    task_id=self.config["task_id"],
                    agent_id="unknown",
                    timestamp=datetime.now(),
                    metrics={"error": "Invalid results format"},
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message="Results must contain 'results' field"
                )
            
            # Check if all images were processed
            expected_images = set(self.config["input_files"])
            processed_images = {r["image"] for r in results["results"]}
            missing_images = expected_images - processed_images
            
            if self.require_all_images and missing_images:
                return EvaluationResult(
                    task_id=self.config["task_id"],
                    agent_id="unknown",
                    timestamp=datetime.now(),
                    metrics={
                        "processed_images": len(processed_images),
                        "missing_images": list(missing_images),
                        "error": "Not all images processed"
                    },
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message=f"Missing {len(missing_images)} images: {missing_images}"
                )
            
            # Calculate metrics
            metrics = self._calculate_metrics(results)
            
            # Determine success
            success = (
                metrics.get("average_iou", 0) >= self.min_iou and
                metrics.get("average_f1", 0) >= self.min_f1_score and
                len(missing_images) == 0
            )
            
            return EvaluationResult(
                task_id=self.config["task_id"],
                agent_id="unknown",
                timestamp=datetime.now(),
                metrics=metrics,
                success=success,
                execution_time=time.time() - start_time,
                error_message=None if success else "Metrics below threshold"
            )
            
        except Exception as e:
            return EvaluationResult(
                task_id=self.config["task_id"],
                agent_id="unknown",
                timestamp=datetime.now(),
                metrics={"error": str(e)},
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"Evaluation failed: {str(e)}"
            )
    
    def _calculate_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate aggregate metrics from individual results."""
        metrics = {}
        
        # Get overall metrics if provided
        if "overall_metrics" in results:
            metrics.update(results["overall_metrics"])
        
        # Calculate from individual results if not provided
        if "results" in results and results["results"]:
            iou_scores = []
            f1_scores = []
            noun_recalls = []
            function_accuracies = []
            
            for result in results["results"]:
                if "metrics" in result:
                    if "iou" in result["metrics"]:
                        iou_scores.append(result["metrics"]["iou"])
                    if "f1_score" in result["metrics"]:
                        f1_scores.append(result["metrics"]["f1_score"])
                    if "noun_recall" in result["metrics"]:
                        noun_recalls.append(result["metrics"]["noun_recall"])
                    if "function_word_accuracy" in result["metrics"]:
                        function_accuracies.append(result["metrics"]["function_word_accuracy"])
            
            # Calculate averages
            if iou_scores:
                metrics["average_iou"] = np.mean(iou_scores)
            if f1_scores:
                metrics["average_f1"] = np.mean(f1_scores)
            if noun_recalls:
                metrics["average_noun_recall"] = np.mean(noun_recalls)
            if function_accuracies:
                metrics["average_function_accuracy"] = np.mean(function_accuracies)
            
            metrics["num_images_processed"] = len(results["results"])
            metrics["num_tokens_analyzed"] = sum(
                len(r.get("token_activations", [])) for r in results["results"]
            )
        
        return metrics
    
    def get_metrics(self) -> List[str]:
        """Returns a list of metric names used in evaluation."""
        return [
            "average_iou",
            "average_f1",
            "average_noun_recall", 
            "average_function_accuracy",
            "num_images_processed",
            "num_tokens_analyzed"
        ]
    
    def generate_report(self, results: List[EvaluationResult]) -> str:
        """Generates a human-readable report from evaluation results."""
        report = []
        report.append("=" * 60)
        report.append(f"TAM Visual Explanation Task Evaluation Report")
        report.append("=" * 60)
        
        for result in results:
            report.append(f"\nAgent: {result.agent_id}")
            report.append(f"Success: {result.success}")
            report.append(f"Execution Time: {result.execution_time:.2f}s")
            
            if result.success:
                metrics = result.metrics
                report.append(f"\nMetrics:")
                report.append(f"  Average IoU: {metrics.get('average_iou', 0):.3f}")
                report.append(f"  Average F1 Score: {metrics.get('average_f1', 0):.3f}")
                report.append(f"  Average Noun Recall: {metrics.get('average_noun_recall', 0):.3f}")
                report.append(f"  Average Function Word Accuracy: {metrics.get('average_function_accuracy', 0):.3f}")
                report.append(f"  Images Processed: {metrics.get('num_images_processed', 0)}")
                report.append(f"  Tokens Analyzed: {metrics.get('num_tokens_analyzed', 0)}")
            else:
                report.append(f"Error: {result.error_message}")
            
            report.append("-" * 60)
        
        return "\n".join(report)
